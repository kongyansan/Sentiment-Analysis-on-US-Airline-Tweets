{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment:confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason:confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>681448150</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 5:24</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681448153</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 1:53</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>681448156</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 10:01</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>681448158</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 3:05</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>681448159</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 5:50</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  681448150    False   finalized                   3      2/25/15 5:24   \n",
       "1  681448153    False   finalized                   3      2/25/15 1:53   \n",
       "2  681448156    False   finalized                   3     2/25/15 10:01   \n",
       "3  681448158    False   finalized                   3      2/25/15 3:05   \n",
       "4  681448159    False   finalized                   3      2/25/15 5:50   \n",
       "\n",
       "  airline_sentiment  airline_sentiment:confidence negativereason  \\\n",
       "0           neutral                        1.0000            NaN   \n",
       "1          positive                        0.3486            NaN   \n",
       "2           neutral                        0.6837            NaN   \n",
       "3          negative                        1.0000     Bad Flight   \n",
       "4          negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason:confidence         airline airline_sentiment_gold  \\\n",
       "0                        NaN  Virgin America                    NaN   \n",
       "1                     0.0000  Virgin America                    NaN   \n",
       "2                        NaN  Virgin America                    NaN   \n",
       "3                     0.7033  Virgin America                    NaN   \n",
       "4                     1.0000  Virgin America                    NaN   \n",
       "\n",
       "         name negativereason_gold  retweet_count  \\\n",
       "0     cairdin                 NaN              0   \n",
       "1    jnardino                 NaN              0   \n",
       "2  yvonnalynn                 NaN              0   \n",
       "3    jnardino                 NaN              0   \n",
       "4    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "   tweet_created      tweet_id tweet_location               user_timezone  \n",
       "0  2/24/15 11:35  5.703060e+17            NaN  Eastern Time (US & Canada)  \n",
       "1  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
       "2  2/24/15 11:15  5.703010e+17      Lets Play  Central Time (US & Canada)  \n",
       "3  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
       "4  2/24/15 11:14  5.703010e+17            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline = pd.read_csv(r\"Airline-Sentiment-2-w-AA.csv\",encoding = 'iso-8859-1')\n",
    "airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'airline_sentiment',\n",
       "       'airline_sentiment:confidence', 'negativereason',\n",
       "       'negativereason:confidence', 'airline', 'airline_sentiment_gold',\n",
       "       'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord',\n",
       "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>681448150</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681448153</td>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>681448156</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>681448158</td>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>681448159</td>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id airline_sentiment         airline  \\\n",
       "0  681448150           neutral  Virgin America   \n",
       "1  681448153          positive  Virgin America   \n",
       "2  681448156           neutral  Virgin America   \n",
       "3  681448158          negative  Virgin America   \n",
       "4  681448159          negative  Virgin America   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = airline[[\"_unit_id\", \"airline_sentiment\", \"airline\", \"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['airline_sentiment']!='neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>681448153</td>\n",
       "      <td>1</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>681448158</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>681448159</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>681448162</td>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>681448165</td>\n",
       "      <td>1</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id airline_sentiment         airline  \\\n",
       "1  681448153                 1  Virgin America   \n",
       "3  681448158                 0  Virgin America   \n",
       "4  681448159                 0  Virgin America   \n",
       "5  681448162                 0  Virgin America   \n",
       "6  681448165                 1  Virgin America   \n",
       "\n",
       "                                                text  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...  \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if row.airline_sentiment == 'positive':\n",
    "        df.at[i,'airline_sentiment'] = 1\n",
    " \n",
    "    else:\n",
    "        df.at[i,'airline_sentiment'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fef18232d30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "airline_sentiment = df.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack()\n",
    "airline_sentiment.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df['text'])\n",
    "target = df['airline_sentiment']\n",
    "processed_features = []\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "for sentence in range(0, len(features)): \n",
    "    \n",
    "    #remove html tags \n",
    "    processed_feature = BeautifulSoup(features[sentence], 'lxml').get_text()\n",
    "    \n",
    "    #remove @mentions\n",
    "    processed_feature = re.sub(r'@[A-Za-z0-9]+','',processed_feature)\n",
    "    \n",
    "    #remove URL links \n",
    "    processed_feature = re.sub('https?://[A-Za-z0-9./]+','',processed_feature)\n",
    "    \n",
    "    #remove symbol #\n",
    "    processed_feature = re.sub(\"[^a-zA-Z]\", \" \", processed_feature)\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(processed_feature))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    \n",
    "    #Spell correction\n",
    "    processed_feature = re.sub(r'(.)\\1+', r'\\1\\1', processed_feature)\n",
    "    \n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' plus you ve added commercials to the experience tacky ',\n",
       " ' it really aggressive to blast obnoxious entertainment in your guests faces they have little recourse',\n",
       " ' and it a really big bad thing about it',\n",
       " ' seriously would pay flight for seats that didn have this playing it really the only bad thing about flying va',\n",
       " ' yes nearly every time fly vx this ear worm won go away ']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embedding(file):\n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = load_embedding('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_voc = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def make_tokenizer(texts, len_voc):\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = make_tokenizer(processed_features, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_token = tokenizer.texts_to_sequences(processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[523, 3, 72, 1211, 2238, 1, 2, 161, 4994],\n",
       " [11, 107, 2976, 1, 3704, 3705, 902, 10, 14, 2977, 3706, 41, 19, 447, 2239],\n",
       " [6, 11, 250, 107, 407, 182, 448, 76, 11],\n",
       " [363,\n",
       "  73,\n",
       "  262,\n",
       "  5,\n",
       "  4,\n",
       "  188,\n",
       "  15,\n",
       "  178,\n",
       "  19,\n",
       "  22,\n",
       "  2016,\n",
       "  11,\n",
       "  107,\n",
       "  2,\n",
       "  106,\n",
       "  182,\n",
       "  448,\n",
       "  76,\n",
       "  133,\n",
       "  1712],\n",
       " [170, 1269, 278, 40, 100, 1598, 22, 3707, 4995, 197, 132, 417]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_token[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_token = pad_sequences(X_token, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {0: ''}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "synonyms_number = 5\n",
    "word_number = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat) \n",
    "neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "synonyms = {x[0]: x[1:] for x in neighbours_mat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selling : ['buying', 'sell', 'sold', 'sale']\n",
      "while : ['and', 'instead', 'though', 'but']\n",
      "didn : ['couldn', 'doesn', 'wouldn', 'wasn']\n",
      "final : ['second', 'round', 'opening', 'first']\n",
      "congrats : ['argh', 'aww', 'woohoo', 'trvl']\n",
      "kid : ['boy', 'dad', 'crazy', 'mom']\n",
      "daughter : ['wife', 'mother', 'sister', 'niece']\n",
      "this : ['it', 'same', 'one', 'only']\n",
      "done : ['sure', 'doing', 'nothing', 'so']\n",
      "movies : ['films', 'movie', 'hollywood', 'film']\n"
     ]
    }
   ],
   "source": [
    "for x in np.random.randint(1, word_number, 10):\n",
    "    print(f\"{index_word[x]} : {[index_word[synonyms[x][i]] for i in range(synonyms_number-1)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.asarray(df.airline_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    2,  161, 4994],\n",
       "       [   0,    0,    0, ...,  197,  132,  417],\n",
       "       [   0,    0,    0, ...,   30,   44,  392],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   69, 1464, 2419],\n",
       "       [   0,    0,    0, ...,   29,  216,    9],\n",
       "       [   0,    0,    0, ...,    5,    1,  442]], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pos = X_token[Y==1]\n",
    "X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_sentence(sentence, synonyms, p=0.5):\n",
    "    for i in range(len(sentence)):\n",
    "        if np.random.random() > p:\n",
    "            try:\n",
    "                syns = synonyms[sentence[i]]\n",
    "                sentence[i] = np.random.choice(syns)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview function \n",
    "indexes = np.random.randint(0, X_pos.shape[0], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thanks\n",
      "spite\n",
      " \n",
      "thank you left my ipad on plane filled out lost and found form yall found it and shipped it back thank you flysw\n",
      "thank you left my ipad on jet filled out lost and found form yall been only and shipped it back thank you flysw\n",
      " \n",
      "thank you my second flight already got bumped up to st love it\n",
      "thank sure my fifth flight has went bumped up to st dream it\n",
      " \n",
      "ll do that can dm until you follow me thanks\n",
      "jj we it can dm then get come know unfortunately\n",
      " \n",
      "has the most incredible customer service ve ever experienced so refreshing\n",
      "has same most incredible customer service ve yet experienced but refreshing\n",
      " \n",
      "just want shout out thank the pilots and staff on the feb flight from newark to boston at pm they were super helpful\n",
      "just do scream out wish the controllers and staff on same feb flight from newark make boston place ends they were super helpful\n",
      " \n",
      "friendly engaging personable handled clarifying questions about baggage fees well and took an interest in what was doing\n",
      "friendly engaging personable handled clarifying questions about checked expenses well well took an interest in what was going\n",
      " \n",
      "great flight\n",
      "little flights\n",
      " \n",
      "great flight experience again sfo jfk dare say it consistent high quality on legacy airline keep it up guys\n",
      "great pilot experience came sfo logan dare say it manner high quality on legacy airlines kept but down guys\n",
      " \n",
      "first time flying virgin went to sanfrancisco thanks for the smooth ride easily my new fav airline\n",
      "first when airplanes branson went would sanfrancisco thanks for same smooth bicycle easily my new hdn lufthansa\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for x in X_pos[indexes]:\n",
    "    sample =  np.trim_zeros(x)\n",
    "    sentence = ' '.join([index_word[x] for x in sample])\n",
    "    print(sentence)\n",
    "\n",
    "    modified = modify_sentence(sample, synonyms)\n",
    "    sentence_m = ' '.join([index_word[x] for x in modified])\n",
    "    print(sentence_m)\n",
    "    \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9178\n",
       "1    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_texts = 6815\n",
    "indexes = np.random.randint(0, X_pos.shape[0], n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  156,  718,  712],\n",
       "       [   0,    0,    0, ..., 4875,    9, 3270],\n",
       "       [   0,    0,    0, ...,  512,   10, 8500],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 1813,   37, 5655],\n",
       "       [   0,    0,    0, ...,   86,  627,   29],\n",
       "       [   0,    0,    0, ...,  424,  152,  311]], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gen_pos = np.array([modify_sentence(x, synonyms) for x in X_pos[indexes]])\n",
    "X_gen_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X, X_gen_pos), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_gen_pos = []\n",
    "\n",
    "for i in range(6815):\n",
    "    y_gen_pos.append(1)\n",
    "    \n",
    "y_gen_pos = np.asarray(y_gen_pos)\n",
    "y_gen_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((Y,y_gen_pos), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18356, 18356)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9178 9178\n"
     ]
    }
   ],
   "source": [
    "print(list(Y).count(1), list(Y).count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =y_train.astype('int')\n",
    "y_test =y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocchio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      1872\n",
      "           1       0.64      0.47      0.54      1800\n",
      "\n",
      "    accuracy                           0.61      3672\n",
      "   macro avg       0.62      0.61      0.60      3672\n",
      "weighted avg       0.62      0.61      0.60      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "text_clf = Pipeline([('clf', NearestCentroid())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79      1872\n",
      "           1       0.77      0.80      0.78      1800\n",
      "\n",
      "    accuracy                           0.79      3672\n",
      "   macro avg       0.79      0.79      0.79      3672\n",
      "weighted avg       0.79      0.79      0.79      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', GradientBoostingClassifier(n_estimators=100)),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.67      0.68      1872\n",
      "           1       0.67      0.70      0.69      1800\n",
      "\n",
      "    accuracy                           0.68      3672\n",
      "   macro avg       0.69      0.69      0.68      3672\n",
      "weighted avg       0.69      0.68      0.68      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', BaggingClassifier(KNeighborsClassifier())),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.46      0.55      1872\n",
      "           1       0.58      0.77      0.66      1800\n",
      "\n",
      "    accuracy                           0.61      3672\n",
      "   macro avg       0.63      0.62      0.61      3672\n",
      "weighted avg       0.63      0.61      0.60      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69      1872\n",
      "           1       0.67      0.72      0.69      1800\n",
      "\n",
      "    accuracy                           0.69      3672\n",
      "   macro avg       0.69      0.69      0.69      3672\n",
      "weighted avg       0.69      0.69      0.69      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', KNeighborsClassifier()),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/chiranmb/miniconda3/envs/py3k/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.58      0.66      1872\n",
      "           1       0.65      0.81      0.72      1800\n",
      "\n",
      "    accuracy                           0.69      3672\n",
      "   macro avg       0.71      0.70      0.69      3672\n",
      "weighted avg       0.71      0.69      0.69      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', SVC()),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      1872\n",
      "           1       0.71      0.73      0.72      1800\n",
      "\n",
      "    accuracy                           0.72      3672\n",
      "   macro avg       0.72      0.72      0.72      3672\n",
      "weighted avg       0.72      0.72      0.72      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', tree.DecisionTreeClassifier()),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83      1872\n",
      "           1       0.81      0.85      0.83      1800\n",
      "\n",
      "    accuracy                           0.83      3672\n",
      "   macro avg       0.83      0.83      0.83      3672\n",
      "weighted avg       0.83      0.83      0.83      3672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "                     ])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
